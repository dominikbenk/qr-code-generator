{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7de9425-b06c-4948-aa88-81b8c401ce74",
   "metadata": {},
   "source": [
    "# Creating Artistic QR Codes at Scale Using LangChain and ControlNet\n",
    "\n",
    "## Summary\n",
    "We built a tool that can generate artistic QR codes for a specific website/url with the use of [DeepLake](https://www.activeloop.ai/), [LangChain](https://python.langchain.com/docs/get_started/introduction.html), [Stable Diffusion](https://github.com/AUTOMATIC1111/stable-diffusion-webui) and [ControlNet](https://github.com/Mikubill/sd-webui-controlnet). \n",
    "\n",
    "DeepLake is a multi-modal vector database, designed to efficiently store and search large-scale AI data including audio, video or embeddings from text documents, which will also be utilized in this article. It offers unique storage optimization for deep learning applications, featuring data streaming, vector search, data versioning, and seamless integration with other popular frameworks such as LangChain. This comprehensive toolkit is designed to simplify the process of developing workflows of large language model (LLM), and in our case we will focus on its capability to summarize and answer questions from large-scale documents such as web pages. \n",
    "\n",
    "Stable diffusion is a recent development in the field of image synthesis, with exciting potential for reducing high computational demand. It is primarily used for text-to-image generation, but is capable of variety of other tasks such as image modification, inpainting, outpainting, upscaling and generating image-to-image conditioned on text input. Meanwhile, ControlNet is an innovative neural network architecture that is a game-changer in managing the control of these diffusion models by integrating extra conditions. These control techniques include edge and line detection, human poses, image segmentation, depth maps, image styles or simple user scribbles. By applying these techniques, it is then possible to condition our output image with QR codes as well. In case you would be interested in more details, we recommend reading the [original ControlNet article](https://arxiv.org/abs/2302.05543).\n",
    "\n",
    "\n",
    "By combining all of this, we can achieve a scalable generation of QR codes that are very unique and more likely will attract attention. Overall, there are many possibilities you can approach this problem, and in this article, we will present those that we believe have the highest potential to impact advertising in the future. These are the steps that we are going to walk you through:\n",
    "\n",
    "## Steps\n",
    "1. Scraping the Content From a Website and Splitting It Into Documents\n",
    "2. Saving the Documents Along With Their Embeddings to Deeplake\n",
    "3. Extracting the Most Relevant Documents\n",
    "4. Creating Prompts to Generate an Image Based on Documents\n",
    "    - 4.1 Custom summary prompt + LLMChain\n",
    "    - 4.2 QA retrieval + LLM\n",
    "5. Summarizing the Created Prompts\n",
    "6. Generating Simple QR From URL\n",
    "7. Generating Artistic QR Codes for Activeloop\n",
    "    - 7.1. Txt2Img\n",
    "        - 7.1.1 Content prompt\n",
    "        - 7.1.2 Portrait prompt\n",
    "        - 7.1.3 Deeplake prompt\n",
    "    - 7.2. Img2Img with logo\n",
    "        - 7.2.1 Content prompt\n",
    "        - 7.2.2 Portrait prompt\n",
    "        - 7.2.3 Deeplake prompt\n",
    "8. Generating Artistic QR Codes for E-commerce\n",
    "    - 8.1. Img2Img with logo - Tommy Hilfiger\n",
    "    - 8.2. Img2Img with logo - Patagonia\n",
    "9. Limitations of Our Approach\n",
    "10. Conclusion\n",
    "        \n",
    "    \n",
    "Before we start, we need to install requirements, import LangChain and set the following API tokens:\n",
    "- Apify token - web scraping/crawling\n",
    "- Activeloop token - vector database\n",
    "- OpenAI token - language model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69054140-e6d1-4b17-82b2-85c1c549f8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "## pip install openai langchain deeplake apify-client tiktoken pydantic==1.10.8\n",
    "\n",
    "# Import libraries\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.utilities import ApifyWrapper\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders.base import Document\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain import PromptTemplate\n",
    "import os\n",
    "\n",
    "# Set API tokens\n",
    "os.environ['OPENAI_API_KEY'] = '<YOUR_OPENAI_TOKEN>'\n",
    "os.environ['ACTIVELOOP_TOKEN'] = '<YOUR_ACTIVELOOP_TOKEN>'\n",
    "os.environ[\"APIFY_API_TOKEN\"] = '<YOUR_APIFY_TOKEN>'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49005f18-3758-4188-a8b3-6024513c2b77",
   "metadata": {},
   "source": [
    "### Step 1: Scraping the Content From a Website and Splitting It Into Documents\n",
    "First of all, we need to collect data that will be used as a content used to generate QR codes. Since the goal is to personalize it to a specific website, we provide a simple pipeline that can crawl data from a given URL. As an example, we use https://www.activeloop.ai/ from which we scraped 20 pages, but you could use any other website as long as it does not violate the Terms of Use. Or, if you wish to use other type of content, \n",
    "LangChain provide many other [File loaders](https://js.langchain.com/docs/modules/indexes/document_loaders/examples/file_loaders/) and [Website loaders](https://js.langchain.com/docs/modules/indexes/document_loaders/examples/web_loaders/) and you can personalize QR codes for them too!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f825bf2-d1bc-4ada-a48d-2593931b5d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use crawler from ApifyWrapper(), which is available in Langchain\n",
    "# For convenience, we set 20 maximum pages to crawl with a timeout of 300 seconds.\n",
    "apify = ApifyWrapper()\n",
    "loader = apify.call_actor(\n",
    "    actor_id=\"apify/website-content-crawler\",\n",
    "    run_input={\"startUrls\": [{\"url\": \"https://www.activeloop.ai/\"}], \"maxCrawlPages\": 20},\n",
    "    dataset_mapping_function=lambda item: Document(\n",
    "        page_content=item[\"text\"] or \"\", metadata={\"source\": item[\"url\"]}\n",
    "    ),\n",
    "    timeout_secs=300,\n",
    ")\n",
    "\n",
    "# Now the pages are loaded and split into chunks with a maximum size of 1000 tokens\n",
    "pages = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0, separator = \".\")\n",
    "docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8768b20-ae0d-418e-b6c4-b0472209a0ab",
   "metadata": {},
   "source": [
    "\n",
    "### Step 2: Saving the Documents Along With Their Embeddings to Deeplake\n",
    "Once the website is scraped and pages are split into documents, it's time to generate the embeddings and save them to the DeepLake. This means that we can come back to our previously scraped data at any time and don't need to recalculate the embeddings again. To do that, you need to set your `ACTIVELOOP_ORGANIZATION_ID`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34562194-71c7-45e2-b17b-a8de17120bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the embedding model\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# initialize the database, can also be used to load the database\n",
    "db = DeepLake(\n",
    "    dataset_path=\"hub://<ACTIVELOOP_ORGANIZATION_ID>/scraped-websites\",\n",
    "    embedding_function=embeddings,\n",
    "    token=os.environ['ACTIVELOOP_TOKEN'],\n",
    "    overwrite=False,\n",
    ")\n",
    "\n",
    "# save the documents\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a862b180-983f-420e-b747-1ff0c836aae1",
   "metadata": {},
   "source": [
    "### Step 3: Extracting the Most Relevant Documents\n",
    "Since we want to generate an image in the context of the given website that can have hundreds of pages, it is useful to filter documents that are the most relevant for our query, in order to save money on chained API calls to LLM. For this, we are going to leverage [Deep Lake Vector Database](https://docs.activeloop.ai/tutorials/vector-store/deep-lake-vector-store-in-langchain) similarity search as well as retrieval functionality.\n",
    "\n",
    "To pre-filter the documents based on a query, we can do the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fa9e2d-1ab1-43a4-80df-83aeab80cc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'You are a prompt generator. Based on the content, write a detailed one sentence description that can be used to generate an image'\n",
    "\n",
    "result = db.similarity_search(query, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab2ccf1-34f7-4d82-a06e-3cff0bb2d381",
   "metadata": {},
   "source": [
    "For question-answering pipeline, we can then define the retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4f4402-94e4-4fbc-b7d0-391c71da06ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(\n",
    "    search_kwargs={\"k\":10}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957d8966-3f2d-466e-b6a0-7fe649014523",
   "metadata": {},
   "source": [
    "### Step 4: Creating Prompts to Generate an Image Based on Documents\n",
    "The goal is to understand the content and generate prompts in an automated way, so that the process can be scalable. We start by initializing the LLM with a default `gpt-3.5-turbo` model and set the high temperature to introduce more randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094a8527-2945-46d9-b346-a4deda173912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM\n",
    "llm = OpenAI(temperature=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201e81e0-6154-4776-850f-e77653da7a3e",
   "metadata": {},
   "source": [
    "One of many advantages of LangChain are also prompt templates, which significantly help with clarity and readability. To make the output description more precise, we should also provide examples as can be seen here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebacb8b3-1019-4cbc-9cd3-0eec4028de79",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"{query}:\n",
    "\n",
    "Example:\n",
    "\n",
    "Content: real estate company\n",
    "Detailed image description of building: Detailed image description: best quality, realistic, photorealistic, ultra detailed, 8K, High quality texture, intricate details, detailed texture, finely detailed, photo of building partial_view, flower pond, tree, no humans, sky, scenery, outdoors, cloud, blue sky, day, palm tree, plant\n",
    "\n",
    "\n",
    "Content: {text}\n",
    "Detailed image description:\n",
    "\"\"\"\n",
    "\n",
    "# set the prompt template\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, \n",
    "    input_variables=[\"text\"], \n",
    "    partial_variables={\"query\": query}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bca5b1-2c4c-4d47-a16c-b3fcbdd1090a",
   "metadata": {},
   "source": [
    "The `query` is identical to the one that is used during similarity search and\n",
    "`text` is the content that is provided to the LLM, based on which it is then supposed to provide a detailed image description. Additionally, to have more control over the output, we also create an alternative prompt that is able to generate specific image type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539120de-e198-4019-bfe7-21c976edd4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_type='portrait'\n",
    "\n",
    "prompt_template = \"\"\"{query}:\n",
    "\n",
    "Example:\n",
    "\n",
    "Content: real estate company\n",
    "Detailed image description of building: Detailed image description: best quality, realistic, photorealistic, ultra detailed, 8K, High quality texture, intricate details, detailed texture, finely detailed, photo of building partial_view, flower pond, tree, no humans, sky, scenery, outdoors, cloud, blue sky, day, palm tree, plant\n",
    "\n",
    "\n",
    "Content: {text}\n",
    "Detailed image description of {image_type}: \n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, \n",
    "    input_variables=[\"text\"], \n",
    "    partial_variables={\"query\": query, \"image_type\": image_type}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444f7bdc-e69e-43c2-965a-75fec42fdda8",
   "metadata": {},
   "source": [
    "Using this, we then experimented with 2 following approaches, that differ in what kind of `text` is provided.\n",
    "\n",
    "\n",
    "#### Option 1: Custom summary prompt with LLMChain\n",
    "\n",
    "The idea is simple, we chain the description prompt on each filtered document and then apply it once again on the summarized descriptions. In other words, `text` will be a variable that is iterated during `LMMChain` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f9d3ad-a5db-4c31-8b3b-53f9bc5cf2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the chain\n",
    "chain = LLMChain(llm=llm, prompt=PROMPT)\n",
    "\n",
    "# Filter the most relevant documents\n",
    "result = db.similarity_search(query, k=10)\n",
    "# Run the Chain\n",
    "chain.run(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88f0309-5cc1-429e-9e1f-b3de6c4ed409",
   "metadata": {},
   "source": [
    "#### Option 2: Retrieval Question-Answering with LLM\n",
    "\n",
    "Here we initialize QA retriever, which will allow us to ask to explain a particular concept on the filtered documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423fa62c-372d-4a30-ad55-b00bfeb4ef12",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type='stuff', \n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "answer = qa.run(\"Explain what is deeplake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fa3227-ac47-47ae-a03e-483f80c36a9f",
   "metadata": {},
   "source": [
    "The `answer` is then used as `text` in the `PromptTemplate` without the need for any chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b7a1b1-ca5a-486f-a5dc-53d37a4b80aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(prompt=PROMPT.format(text=answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e281b59d-7814-4889-b724-22a7a07dc166",
   "metadata": {},
   "source": [
    "### Step 5: Summarizing the Created Prompts\n",
    "We experimented with different prompt setups in the previous section, and yet there is more to explore. In case you would be interested in perfectionizing your LLM prompts even further, we have an [amazing course](https://learn.activeloop.ai/courses/take/langchain/multimedia/46317727-intro-to-prompt-engineering-tips-and-tricks) that will provide you many useful tips and tricks. Mastering prompts for image generation is, however, more art than science. Nevertheless, by providing the LLM with examples we can see that it can do a pretty good job by generating very specific image descriptions. Here are 3 different types of prompts that we were able to generate with our approach:\n",
    "\n",
    "#### 1. Content prompt\n",
    "This prompt summarizes all relevant documents scraped from Activeloop into a general but detailed image description: `high-tech, futuristic, AI-driven, advanced, complex, computer-generated, robot, machine learning, data visualization, interactive, cutting-edge technology, automation, precision, efficiency, innovation, digital transformation, smart technology, science fiction-inspired.\n",
    "`\n",
    "\n",
    "#### 2. Portrait prompt\n",
    "Additionally to previous prompt, we also condition on the type of the image, which is in this case a detailed image description of a portrait: `High quality portrait of a developer working on LangChain, surrounded by computer screens and programming tools, with a focus on the keyboard and coding on the screen.\n",
    "`\n",
    "\n",
    "#### 3. DeepLake prompt\n",
    "Here we show a Question-Answering example with a detailed image description of DeepLake: `An aerial view of a serene, glassy lake surrounded by trees and mountains, with giant blocks of data floating on the surface, each block representing a different data type such as images, videos, audio, and tabular data, all stored as tensors, while a team of data scientists in a nearby cabin focus on their work to build advanced deep learning models, powered by GPUs that are seamlessly integrated with Deep Lake.\n",
    "`\n",
    "\n",
    "\n",
    "### Step 6: Generating Simple QR From URL\n",
    "Before we generate the art, it is important to prepare the simple QR code for ControlNet, which can be done for example [here](https://keremerkan.net/qr-code-and-2d-code-generator/). It is important to set the error correction level to 'H', which increases the probability of QR being readable, as 30% of the code can be covered/destroyed by an image. To generate QR code with a logo, we can then use [this website](https://www.qrcode-monkey.com/) for example. It is also important to note, that some of the URLs might be too long to generate a QR that is not too complicated and reliable enough for scanning. For this purpose, we can use url shorteners such as [bit.ly](bit.ly).\n",
    "\n",
    "### Step 7: Generating Artistic QR Codes for Activeloop\n",
    "First of all, we need to keep in mind that it is still very fresh and unexplored topic and the more pleasing-looking QRs you want to generate, the higher risk of not being readable by a scanner. This results in an endless cycle of adjusting parameters to find the most general setup. Many approaches can be applied, but their main difference is in ControlNet units. The highest success we had was with [brightness and tile preprocessors](https://huggingface.co/lllyasviel/ControlNet-v1-1/tree/main), as well as the [qrcode preprocessor](https://huggingface.co/DionTimmer/controlnet_qrcode). Sometimes, adding a depth preprocessor was also helpful. A great guide on how to set up the Stable-diffusion webui with ControlNet extension to generate your first QR codes can be found for example [here](https://www.youtube.com/watch?v=HOY5J9UT_lY). Nevertheless, there is no single setup that would work 100% of the time and a lot of experimenting is needed, especially in terms of finetuning the control's strength/start/end to achieve a desirable output.\n",
    "\n",
    "For example, in most of the QR codes we used the following setup:\n",
    "- Negative prompt: ugly, disfigured, low quality, blurry, nsfw\n",
    "- Steps: 20\n",
    "- Sampler: DPM++ 2M Karras\n",
    "- CFG scale: 9\n",
    "- Size: 768x768\n",
    "- Model: dreamshaper_631BakedVae\n",
    "- ControlNet\n",
    "    - 0: preprocessor: none, model: control_v1p_sd15_qrcode, weight: 1.1, starting/ending: (0, 1), resize mode: Crop and Resize, pixel perfect: False, control mode: Balanced\n",
    "    - 1: preprocessor: none, model: control_v1p_sd15_brightness, weight: 0.3, starting/ending: (0, 1), resize mode: Crop and Resize, pixel perfect: False, control mode: Balanced\n",
    "    \n",
    "In case of Img2Img, we would also need to put an inpaint mask to disable any changes to the logo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34806367-7751-4fa7-9003-32eea8d9779d",
   "metadata": {},
   "source": [
    "#### Txt2Img - generating QR code from a simple QR and previously created prompt\n",
    "##### Content prompt\n",
    "| | |\n",
    "|:-------------------------:|:-------------------------:|\n",
    "|<img width=\"768\" src=\"images/7.1.1.1.png\"> | <img width=\"768\" src=\"images/7.1.1.2.png\">|\n",
    "|<img width=\"768\" src=\"images/7.1.1.3.png\"> | <img width=\"768\" src=\"images/7.1.1.4.png\">|\n",
    "\n",
    "##### Portrait prompt\n",
    "| | |\n",
    "|:-------------------------:|:-------------------------:|\n",
    "|<img width=\"768\" src=\"images/7.1.2.1.png\"> | <img width=\"768\" src=\"images/7.1.2.2.png\">|\n",
    "|<img width=\"768\" src=\"images/7.1.2.3.png\"> | <img width=\"768\" src=\"images/7.1.2.4.png\">|\n",
    "\n",
    "##### Deeplake prompt\n",
    "| | |\n",
    "|:-------------------------:|:-------------------------:|\n",
    "|<img width=\"768\" src=\"images/7.1.3.1.png\"> | <img width=\"768\" src=\"images/7.1.3.2.png\">|\n",
    "|<img width=\"768\" src=\"images/7.1.3.3.png\"> | <img width=\"768\" src=\"images/7.1.3.4.png\">|\n",
    "        \n",
    "#### Img2Img with logo - generating QR code from a QR with logo and previously created prompt\n",
    "##### Content prompt\n",
    "| | |\n",
    "|:-------------------------:|:-------------------------:|\n",
    "|<img width=\"768\" src=\"images/7.2.1.1.png\"> | <img width=\"768\" src=\"images/7.2.1.2.png\">|\n",
    "|<img width=\"768\" src=\"images/7.2.1.3.png\"> | <img width=\"768\" src=\"images/7.2.1.4.png\">|\n",
    "\n",
    "##### Portrait prompt\n",
    "| | |\n",
    "|:-------------------------:|:-------------------------:|\n",
    "|<img width=\"768\" src=\"images/7.2.2.1.png\"> | <img width=\"768\" src=\"images/7.2.2.2.png\">|\n",
    "|<img width=\"768\" src=\"images/7.2.2.3.png\"> | <img width=\"768\" src=\"images/7.2.2.4.png\">|\n",
    "\n",
    "##### Deeplake prompt\n",
    "| | |\n",
    "|:-------------------------:|:-------------------------:|\n",
    "|<img width=\"768\" src=\"images/7.2.3.1.png\"> | <img width=\"768\" src=\"images/7.2.3.2.png\">|\n",
    "|<img width=\"768\" src=\"images/7.2.3.3.png\"> | <img width=\"768\" src=\"images/7.2.3.4.png\">|\n",
    "\n",
    "\n",
    "\n",
    "### Step 8: Generating Artistic QR Codes for E-commerce\n",
    "\n",
    "The idea here is a little different compared to the previous examples in context of [Activeloop](activeloop.com).\n",
    "Now, we focus on product advertising and we want to generate a QR code only for a single URL and its product. The challenge is to generate QR code, while also keeping the product as similar to the original as possible to avoid misleading information. To do this, we experimented with many preprocessors such as the `tile`, `depth`, `reference_only`, `lineart` or `styles`, but we found most of them too unreliable and far from being similar to the original input. At this moment, we believe that the most useful  is the `tile` preprocessor, which can preserve a lot of information. The disadvantage is, however, that it does not allow for many changes during control phase and the QR fit can sometimes be questionable. In practice, this would be done by adding another CotntrolNet unit:\n",
    "- 2: preprocessor: none, model: control_v11f1e_sd15_tile, weight: 1.0, starting/ending: (0, 1), resize mode: Crop and Resize, pixel perfect: False, control mode: Balanced\n",
    "Since the `tile` input image control is very strong, theres not much else we can do. Styles are one of the little extra adjustments possible and very useful style cheat sheet can be found [here](https://supagruen.github.io/StableDiffusion-CheatSheet/). For our purposes, however, we did not end up utilizing any of them. \n",
    "\n",
    "\n",
    "Similarly as before, we generated prompts automaticaly from the given websites. We randomly selected 2 products and in the first case (Tommy Hilfiger) We added logo to the initial basic QR code while in the second case (Patagonia), we only mask the logo that is already present on the product. To see the comparison, we also provide the original input images (Sources: [Patagonia](https://eu.patagonia.com/cz/en/product/mens-capilene-cool-daily-graphic-shirt/45235.html?dwvar_45235_color=SSMX&cgid=mens-shirts-tech-tops), [Tommy Hilfiger](https://uk.tommy.com/tommy-hilfiger-x-vacation-flag-embroidery-t-shirt-mw0mw33438ybl)).\n",
    "\n",
    "#### Img2Img with logo - generating Tommy Hilfiger QR code\n",
    "\n",
    "| | |\n",
    "|:-------------------------:|:-------------------------:|\n",
    "| <img width=\"768\" src=\"images/8.1.1.png\"> | <img width=\"768\" src=\"images/8.1.2.png\"> |\n",
    "| <img width=\"768\" src=\"images/8.1.3.png\"> | <img width=\"768\" src=\"images/8.1.4.png\"> |\n",
    "\n",
    "#### Img2Img with logo - generating Patagonia QR code\n",
    "| | |\n",
    "|:-------------------------:|:-------------------------:|\n",
    "| <img width=\"768\" src=\"images/8.2.1.png\"> | <img width=\"768\" src=\"images/8.2.2.png\"> |\n",
    "| <img width=\"768\" src=\"images/8.2.3.png\"> | <img width=\"768\" src=\"images/8.2.4.png\"> |\n",
    "\n",
    "\n",
    "### Limitations of Our Approach\n",
    "- Overall, the ControlNet model required extensive manual tuning of parameters. There are many methods to control the QR code generation process, but none are entirely reliable. The problem intensifies when you want to account for the input product image as well. To the best of our knowledge, no other publication has found a way to generate them reliably, and we spent the majority of our time experimenting with various setups.\n",
    "\n",
    "- Adding an image to the input might offer more control and bring about various use-cases, but it significantly restricts the possibilities of stable diffusion. This usually only results in changes to the image's style without fitting much of the QR structure. Moreover, we saw greater success with text-to-image compared to image-to-image with logo masks. However, the former wasn't as desirable because we believe logos are essential in product QR codes.\n",
    "\n",
    "- From our examples, it's evident that the generated products don't exactly match the actual products one-to-one. If the goal is to advertise a specific product, even a minor mismatch could be misleading. Nonetheless, we believe that [LORA](https://stable-diffusion-art.com/lora/) models or a different type of preprocessor model could address these issues.\n",
    "\n",
    "- Automated image prompts can sometimes be confusing, drawing focus to unimportant details within the context. This is particularly problematic if we don't have enough relevant textual information to build upon. This presents an opportunity to further use the DeepLake's vector DB to analyze the image bind embeddings for a better understanding of the content on e-commerce websites.\n",
    "\n",
    "- In our examples, we also encountered issues with faces, as they sometimes didn't appear human. However, this could be easily addressed with further processing. In instances where we want to preserve the face and adjust it to the QR code, there are tools like the [Roop](https://github.com/s0md3v/sd-webui-roop) that can be used for a detailed face replacement.\n",
    "\n",
    "\n",
    "### Conclusion: Scalable Prompt Generation Achieved, QR Code Generation Remains Unreliable\n",
    "DeepLake combined with LangChain can significantly reduce the costs of analyzing the contents of a website to provide image descriptions in a scalable way. Thanks to the vector database, we can save a large number of documents and images along with their embeddings. This allows us to iteratively adjust the image prompts and efficiently filter based on embedding similarities. However, it is very difficult to find the ControlNet sweet spot of QR readability and \"cool\" design. Taking into account all of the limitations we've discussed, we believe that there needs to be more experimenting with ControlNet, in order to generated product QR codes that are reliable and applicable for real-world businesses.\n",
    "\n",
    "I hope that you find this useful and already have many ideas on how to further build on this. Thank you for reading and I wish you a great day and see you in the next one.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
